{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "%%capture\n",
    "!pip install ipywidgets --upgrade"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at DeepPavlov/rubert-base-cased were not used when initializing BertModel: ['cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model downloaded\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=tensor([[[ 0.0699, -0.0093, -0.2027,  ..., -0.0275,  0.0105, -0.2243],\n",
       "         [ 0.1231, -0.3886, -0.1811,  ..., -0.1576, -0.2227, -0.7154],\n",
       "         [ 0.1322, -0.2026, -0.1001,  ...,  0.0254, -0.0627, -0.7906],\n",
       "         [-0.0746, -0.0255,  0.6633,  ..., -0.1301, -0.2033, -0.4783],\n",
       "         [ 0.3048,  0.3046, -0.2457,  ..., -0.4703, -0.2132,  0.4014]]],\n",
       "       device='cuda:0', grad_fn=<NativeLayerNormBackward0>), pooler_output=tensor([[ 2.1306e-01, -5.7159e-02,  1.9231e-01,  1.0838e-01, -1.3495e-01,\n",
       "          3.2246e-01,  1.4012e-01,  1.1668e-01, -2.2484e-01,  2.4496e-01,\n",
       "         -7.5188e-02, -1.7386e-01, -2.5435e-01, -1.5678e-01,  9.6359e-02,\n",
       "         -2.6535e-01,  7.8951e-01,  9.4643e-02,  1.0328e-01, -1.8292e-01,\n",
       "         -9.9998e-01, -6.9320e-02, -2.4173e-01, -1.1112e-01, -2.8712e-01,\n",
       "          8.0301e-02, -1.3064e-01,  1.0944e-01,  1.0707e-01, -1.2861e-01,\n",
       "          3.6611e-02, -9.9989e-01,  5.3055e-01,  6.4786e-01,  2.0832e-01,\n",
       "         -8.2636e-02,  1.5151e-01,  2.1333e-01,  2.5899e-01, -2.3660e-01,\n",
       "         -1.2510e-01,  4.4086e-02, -1.2929e-01,  1.1797e-01, -1.7334e-01,\n",
       "         -2.3338e-01, -1.0166e-01,  1.3017e-01, -2.5970e-01,  1.4900e-01,\n",
       "         -1.3598e-01,  1.5868e-01,  2.9983e-01,  3.3612e-01,  3.1890e-01,\n",
       "          2.1312e-01,  2.9633e-01,  1.5969e-01,  2.1605e-01,  1.3479e-02,\n",
       "          9.4094e-02,  2.1585e-01,  1.7058e-01, -1.6869e-01, -2.1244e-01,\n",
       "         -1.6219e-01,  1.4524e-01, -1.5018e-01,  5.9466e-01, -2.0419e-01,\n",
       "         -1.5194e-01, -3.3888e-01, -1.8207e-01,  1.9767e-01,  1.2553e-01,\n",
       "         -1.5165e-01,  1.8729e-01,  2.3603e-01,  1.5239e-01, -1.5310e-01,\n",
       "         -2.8070e-01, -4.3740e-01, -1.6631e-01,  6.7715e-02, -1.5038e-01,\n",
       "          2.6956e-02,  2.1367e-01, -2.5813e-01,  1.4830e-01, -1.0009e-02,\n",
       "          1.7197e-01,  4.6085e-01, -1.3515e-01,  2.7430e-01, -1.5200e-01,\n",
       "         -1.3568e-01, -8.9711e-01, -9.8405e-02, -8.7708e-02, -3.5119e-01,\n",
       "         -8.4659e-02,  1.3822e-01, -1.9147e-01, -1.6112e-01, -2.2394e-01,\n",
       "         -7.1606e-02,  1.6457e-01,  2.5532e-01, -1.3777e-01,  1.8225e-01,\n",
       "          1.0081e-01, -3.1835e-01, -1.4554e-01,  1.7206e-01, -2.1699e-01,\n",
       "          9.7634e-01,  1.0954e-01,  1.3956e-01,  2.1370e-02, -6.9990e-02,\n",
       "         -5.9068e-01,  9.9995e-01,  1.0675e-01, -1.2967e-01,  1.7128e-01,\n",
       "          1.2295e-01, -3.1089e-01,  1.2143e-01,  1.7916e-01,  2.1584e-01,\n",
       "          1.1036e-01, -8.3765e-02, -2.0606e-01, -2.5542e-01, -8.7048e-01,\n",
       "         -1.4591e-01, -4.3543e-02,  1.3015e-01, -3.4442e-01, -1.1074e-01,\n",
       "          1.4095e-01,  3.8661e-01,  9.8599e-02, -9.8567e-02, -1.1219e-01,\n",
       "         -1.3977e-01,  6.2703e-02, -2.6061e-01,  9.9994e-01,  7.1575e-01,\n",
       "         -1.1313e-01, -1.9707e-01,  2.0299e-01, -5.6288e-01, -2.3434e-01,\n",
       "         -1.4931e-01, -1.7931e-01, -4.2854e-01,  1.1980e-01,  1.5466e-01,\n",
       "          1.5474e-01, -9.3934e-02, -1.2694e-01, -1.1060e-01,  1.9517e-01,\n",
       "         -5.4481e-01, -2.1796e-01,  1.4773e-01,  1.5813e-01,  1.4659e-01,\n",
       "         -1.0707e-01,  1.8544e-01,  1.2372e-01, -1.6064e-01, -6.7639e-02,\n",
       "          1.2258e-01,  2.2365e-01, -4.7680e-02, -8.2057e-03, -1.0035e-01,\n",
       "          1.3813e-01, -8.2559e-02, -2.7019e-01,  1.2988e-01, -1.1835e-01,\n",
       "         -3.3260e-01,  1.4438e-01, -1.0466e-01, -9.5351e-02,  1.5765e-01,\n",
       "         -6.5672e-02,  1.3510e-01, -1.9694e-01,  1.3515e-01,  5.6633e-03,\n",
       "          1.0482e-01, -2.9727e-01,  1.7412e-01,  1.9937e-01,  1.8860e-01,\n",
       "          1.8999e-02,  2.0632e-01,  3.7139e-02,  1.1800e-01, -5.7556e-02,\n",
       "         -4.3070e-01,  1.2784e-01,  1.0998e-01,  2.3527e-01, -2.3940e-01,\n",
       "         -3.0105e-01, -1.5728e-01,  5.6044e-01,  1.7127e-01, -1.9563e-01,\n",
       "          2.0843e-01,  1.6896e-01, -1.2416e-01, -1.2051e-01,  1.1471e-01,\n",
       "         -7.0752e-02, -1.5122e-01, -3.4134e-01, -9.0308e-02, -1.2231e-01,\n",
       "          1.5158e-01,  1.0112e-01,  1.2558e-01,  7.8914e-02, -1.1393e-01,\n",
       "         -1.2471e-01, -4.7724e-02, -1.4993e-01,  3.2860e-01, -1.4688e-01,\n",
       "          8.7621e-01, -2.3132e-01,  9.0505e-02, -3.0327e-01, -9.8968e-02,\n",
       "          1.6721e-01, -1.9047e-01,  1.8848e-01,  9.5690e-01, -1.7802e-02,\n",
       "         -3.1403e-01,  1.7737e-01,  2.0518e-01,  1.5219e-01, -1.8078e-01,\n",
       "          1.1236e-01, -6.1347e-01,  4.3987e-01,  1.8984e-01,  2.0127e-01,\n",
       "         -9.9998e-01,  7.7356e-02,  1.2901e-01,  1.7773e-01,  2.2155e-01,\n",
       "          1.8863e-01,  1.9290e-01,  2.3247e-01,  9.2073e-01, -3.5862e-01,\n",
       "         -3.2124e-01,  2.9084e-02, -5.5175e-02, -3.0083e-01, -1.4638e-01,\n",
       "         -4.2915e-02, -1.9222e-01, -2.1468e-01,  1.0431e-01, -1.3715e-01,\n",
       "          1.6921e-01,  2.1323e-01, -9.8774e-01,  9.0631e-01,  1.4474e-01,\n",
       "         -1.4758e-01,  7.0807e-02,  6.3968e-02, -9.9998e-01,  1.9915e-01,\n",
       "         -1.6165e-01, -1.5368e-01,  1.4395e-01, -3.7857e-01, -2.0815e-01,\n",
       "          4.9980e-02,  2.3267e-01,  1.5992e-01,  1.8124e-01,  2.0819e-01,\n",
       "          4.2153e-01, -6.2123e-02,  5.0016e-02,  1.1595e-01, -5.5280e-02,\n",
       "          4.0791e-01,  9.2352e-04,  8.1532e-02,  2.9600e-01, -1.1531e-01,\n",
       "          1.9221e-01, -1.5630e-01,  2.1830e-01,  1.7682e-01,  1.3337e-01,\n",
       "          3.6125e-02, -1.0730e-01,  1.4199e-01, -7.9203e-01,  1.1058e-01,\n",
       "         -3.1149e-01,  1.0996e-01, -1.5208e-01,  2.0885e-01, -1.5762e-01,\n",
       "         -1.9352e-01,  8.5411e-02, -7.0674e-02,  9.9997e-01,  2.3251e-01,\n",
       "          4.3549e-02,  1.5201e-01,  3.5817e-01,  1.6757e-01, -1.5791e-01,\n",
       "         -4.3377e-01, -1.7854e-01,  5.7628e-01,  2.0392e-01,  1.1677e-01,\n",
       "          2.2081e-02,  8.6586e-04,  2.4598e-01, -7.6748e-02, -1.6909e-01,\n",
       "          5.2865e-02, -3.1098e-01,  3.2506e-02, -5.3636e-02, -1.9601e-01,\n",
       "          7.6104e-02, -1.0954e-01, -1.9286e-01, -7.6318e-01,  2.7481e-01,\n",
       "         -1.4222e-02,  1.9292e-01,  9.0565e-02,  1.2858e-01, -2.0610e-01,\n",
       "          3.8210e-01,  1.5610e-01, -1.0597e-01, -1.7020e-01, -1.6271e-01,\n",
       "         -2.2793e-01,  1.0289e-01, -1.5652e-01, -3.4200e-01,  1.6654e-01,\n",
       "         -7.2291e-01,  1.3770e-01, -1.0817e-01, -1.6981e-01, -2.9478e-01,\n",
       "          1.3701e-01, -9.9894e-01, -1.4226e-01,  1.9021e-01, -2.4032e-01,\n",
       "          1.2888e-01, -2.3787e-01, -9.5159e-02,  2.1462e-01,  1.6801e-01,\n",
       "         -6.0848e-02,  1.1482e-01, -1.8895e-01,  4.2623e-02, -1.7909e-01,\n",
       "          4.9333e-02,  8.7588e-01,  4.7139e-01,  3.7776e-02, -1.2798e-01,\n",
       "          9.0129e-02, -3.8182e-01, -4.5905e-02,  3.3874e-01,  1.1925e-01,\n",
       "         -1.5559e-01,  1.3037e-01,  1.4582e-01,  1.1049e-01, -1.3671e-01,\n",
       "          1.4473e-01, -1.2377e-01, -3.3697e-02,  1.6050e-01, -1.5492e-01,\n",
       "         -1.6029e-01,  2.0667e-01,  1.4988e-01, -4.0366e-01,  3.1633e-01,\n",
       "          1.7827e-01,  2.3673e-01,  1.4429e-01,  3.4482e-01, -1.8908e-01,\n",
       "         -7.7404e-02, -1.1023e-01,  1.1247e-01, -2.2090e-01, -9.8843e-02,\n",
       "         -2.9815e-02,  9.9965e-01,  2.3651e-01,  2.3564e-01, -1.6291e-01,\n",
       "          1.8526e-01,  1.8207e-01, -2.2875e-01,  2.4794e-01,  1.6595e-01,\n",
       "          1.3582e-01, -1.6585e-01,  2.0089e-02,  1.1696e-01,  1.4398e-01,\n",
       "          2.1730e-01, -9.5978e-02,  4.6571e-01, -2.2687e-01,  7.8925e-01,\n",
       "         -1.1370e-01, -1.9769e-01, -9.9108e-01,  1.5337e-01,  2.8186e-01,\n",
       "         -2.4924e-01, -2.5997e-01,  2.0109e-01, -1.8893e-01,  8.7904e-02,\n",
       "         -1.3815e-01,  5.7404e-02,  1.4209e-01, -1.6756e-01,  2.4270e-01,\n",
       "         -1.6288e-01,  9.9831e-01, -1.3195e-01,  1.4116e-01,  1.5382e-01,\n",
       "          1.2329e-01, -1.3862e-01, -1.7375e-01, -1.1270e-01,  1.7659e-01,\n",
       "         -2.4892e-02,  1.0086e-01, -9.5265e-01,  1.5542e-01,  1.7540e-01,\n",
       "          2.5311e-01, -2.8534e-02,  2.8906e-01, -2.9768e-01,  1.6024e-01,\n",
       "          2.9915e-02, -1.2023e-01, -1.1544e-01,  1.6553e-01, -1.5937e-01,\n",
       "          2.4557e-01, -1.2905e-01,  1.9174e-01, -1.8450e-01,  2.7693e-01,\n",
       "         -1.4398e-01,  2.2690e-01, -9.0265e-02,  2.8521e-01, -1.9331e-01,\n",
       "         -1.9412e-01, -2.0961e-01,  1.6037e-01, -3.5908e-01,  9.9998e-01,\n",
       "         -1.2329e-01,  1.0266e-01, -1.5724e-01,  1.0763e-01, -1.0462e-01,\n",
       "          2.1742e-01,  6.9879e-01, -3.2544e-01,  1.6156e-01,  2.1671e-01,\n",
       "         -7.3948e-01,  2.5752e-02, -9.6392e-02, -7.5250e-01, -7.0015e-02,\n",
       "          9.4384e-01,  1.3505e-01,  2.1146e-01,  1.0470e-01,  3.7975e-01,\n",
       "         -1.8697e-01, -2.0798e-01,  1.1873e-01,  8.7133e-01,  6.3055e-02,\n",
       "          1.8762e-01,  1.4124e-01,  3.0736e-03, -2.4724e-01, -1.5001e-01,\n",
       "          9.9992e-01,  9.9825e-01,  1.1109e-01,  1.7070e-01, -2.9660e-01,\n",
       "         -2.8721e-01, -1.3463e-01,  1.4353e-01,  2.2030e-01,  1.0893e-01,\n",
       "         -2.0922e-01,  1.3230e-01, -2.9007e-01, -1.7897e-01, -2.1197e-01,\n",
       "         -8.6757e-02, -1.7992e-01,  1.8651e-01, -1.9945e-01,  5.9663e-01,\n",
       "          3.2148e-01,  1.3966e-01,  3.3284e-01,  1.3888e-02,  2.0448e-01,\n",
       "         -1.6543e-01, -2.2170e-01,  2.9990e-01, -1.4454e-01, -1.2214e-01,\n",
       "         -3.1304e-01,  7.7656e-02, -9.9998e-01, -1.7321e-01, -1.4866e-01,\n",
       "         -1.8007e-01,  3.2967e-01,  1.1264e-01,  1.6036e-01, -2.3157e-01,\n",
       "         -2.1033e-01, -1.6318e-01,  1.1700e-01,  1.6265e-01,  1.0910e-01,\n",
       "         -1.7344e-01, -2.7230e-01,  2.4638e-01, -3.5355e-01,  1.6477e-01,\n",
       "         -1.5638e-01, -6.9406e-02, -7.9693e-01, -1.1982e-01, -8.1764e-02,\n",
       "          2.2990e-01,  1.3231e-01, -2.1461e-01,  2.4941e-01,  1.6631e-01,\n",
       "          1.4603e-01, -1.8059e-01,  1.5368e-01, -1.3865e-01,  1.2979e-01,\n",
       "          1.8226e-01,  2.6955e-01,  1.8534e-01, -1.0540e-01, -1.1719e-01,\n",
       "         -1.2497e-01, -7.1030e-02, -2.1878e-01,  1.1202e-01, -1.7616e-01,\n",
       "          1.4967e-01, -1.6614e-01,  1.6502e-01, -1.8360e-01,  7.5973e-02,\n",
       "          1.2316e-01,  3.0868e-01, -2.2793e-01,  3.8338e-01,  1.9554e-01,\n",
       "         -1.2756e-01,  3.0177e-01,  4.7191e-02, -2.9988e-01, -1.4168e-01,\n",
       "          9.9998e-01,  3.0346e-01,  9.4563e-02,  2.4617e-01, -2.2630e-01,\n",
       "          2.9400e-01,  8.3260e-02,  4.1262e-01, -9.2678e-02,  8.1239e-01,\n",
       "         -2.5075e-01,  8.4313e-02,  1.1641e-01,  2.6303e-01,  1.5462e-01,\n",
       "          1.8716e-01,  2.7126e-01,  8.1093e-01,  1.7449e-01,  1.6522e-01,\n",
       "          2.5024e-01,  2.4068e-01,  1.6884e-01,  3.1346e-01,  1.8435e-01,\n",
       "          1.3067e-01,  2.8030e-01,  1.0986e-03,  5.5194e-02,  2.0982e-01,\n",
       "         -7.3396e-02, -1.1407e-01, -6.1454e-02, -2.3063e-01,  1.0371e-01,\n",
       "         -5.3679e-02, -1.2595e-01, -6.9116e-02,  1.6449e-01, -1.3850e-01,\n",
       "          2.4342e-01, -9.2534e-02, -2.7312e-01,  5.8665e-01, -2.7470e-01,\n",
       "          1.0470e-01, -1.3967e-01,  1.1231e-01, -8.7061e-01,  1.3345e-01,\n",
       "         -1.2993e-01, -3.4348e-01, -9.3434e-02, -1.3941e-01,  1.6212e-01,\n",
       "          2.2082e-01, -5.6870e-02,  2.0900e-01, -4.5177e-02,  7.9236e-02,\n",
       "         -1.7286e-01, -1.5702e-01,  4.3162e-02, -9.9995e-01,  1.4028e-01,\n",
       "          1.0242e-01, -3.1185e-01,  2.5082e-02,  6.6813e-02,  6.3025e-02,\n",
       "          2.0979e-01, -1.6249e-01, -5.9402e-02, -5.9854e-02,  3.3282e-01,\n",
       "         -1.3096e-01, -2.4643e-01,  6.9674e-02, -1.8561e-01, -2.3832e-01,\n",
       "          8.4835e-02, -1.5910e-01,  1.5544e-01,  1.3868e-01, -1.1915e-01,\n",
       "          1.6934e-02, -1.7843e-01,  1.1440e-01, -1.4067e-01,  8.4096e-02,\n",
       "         -2.6825e-01, -2.4040e-01,  1.6701e-01, -3.7165e-01, -3.5574e-01,\n",
       "         -8.6753e-02,  2.9842e-02, -4.8905e-02,  7.2210e-02,  2.0480e-01,\n",
       "         -1.3680e-01,  1.5270e-01, -1.0333e-01,  2.5095e-01, -1.6811e-01,\n",
       "          1.3087e-01, -8.8351e-01, -1.4631e-01, -1.9963e-01, -1.5271e-02,\n",
       "          2.4537e-01,  2.1313e-01,  1.9975e-01,  1.6516e-01, -4.5182e-02,\n",
       "          1.2818e-01, -1.2218e-01,  1.9090e-01,  1.8737e-01, -1.1517e-01,\n",
       "          9.0050e-02, -1.7125e-01,  1.7600e-01, -1.3598e-01,  1.0584e-01,\n",
       "         -9.8477e-01, -1.5758e-01,  2.2561e-02,  1.0975e-01,  1.1952e-01,\n",
       "         -1.7934e-02,  1.1687e-02, -1.8816e-01, -1.2368e-01,  1.4248e-01,\n",
       "          7.0037e-02,  1.1023e-01,  1.4808e-01,  1.1004e-01, -1.3637e-01,\n",
       "         -5.7034e-02,  7.4251e-01, -1.6586e-01,  1.5859e-01,  1.7149e-01,\n",
       "          1.0176e-01,  7.5346e-01,  1.9455e-01,  2.3841e-01,  1.6356e-01,\n",
       "         -1.9027e-01,  2.1493e-01,  1.7713e-01]], device='cuda:0',\n",
       "       grad_fn=<TanhBackward0>), hidden_states=None, past_key_values=None, attentions=None, cross_attentions=None)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import generic wrappers\n",
    "from transformers import AutoModel, AutoTokenizer \n",
    "\n",
    "# Define the model repo\n",
    "model_name = \"DeepPavlov/rubert-base-cased\" \n",
    "\n",
    "\n",
    "# Download pytorch model\n",
    "model = AutoModel.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "print('model downloaded')\n",
    "\n",
    "# Transform input tokens \n",
    "inputs = tokenizer(\"Hello world!\", return_tensors=\"pt\").to('cuda')\n",
    "\n",
    "# Model apply\n",
    "model.to('cuda')\n",
    "model.eval()\n",
    "outputs = model(**inputs)\n",
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(312,)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny\")\n",
    "# model.cuda()  # uncomment it if you have a GPU\n",
    "\n",
    "def embed_bert_cls(text, model, tokenizer):\n",
    "    t = tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n",
    "    with torch.no_grad():\n",
    "        model_output = model(**{k: v.to(model.device) for k, v in t.items()})\n",
    "    embeddings = model_output.last_hidden_state[:, 0, :]\n",
    "    embeddings = torch.nn.functional.normalize(embeddings)\n",
    "    return embeddings[0].cpu().numpy()\n",
    "\n",
    "print(embed_bert_cls('привет мир', model, tokenizer).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at sberbank-ai/ruBert-base were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Hardware accelerator e.g. GPU is available in the environment, but no `device` argument is passed to the `Pipeline` object. Model will be on CPU.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Python - это самый популярный язык программирования.',\n",
       " 'Python - это самый современный язык программирования.',\n",
       " 'Python - это самый сложный язык программирования.',\n",
       " 'Python - это самый простой язык программирования.',\n",
       " 'Python - это самый распространённый язык программирования.']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import BertForMaskedLM,BertTokenizer, pipeline\n",
    "model=BertForMaskedLM.from_pretrained('sberbank-ai/ruBert-base')\n",
    "tokenizer=BertTokenizer.from_pretrained('sberbank-ai/ruBert-base', do_lower_case=False)\n",
    "unmasker = pipeline('fill-mask', model=model,tokenizer=tokenizer)\n",
    "\n",
    "[i['sequence'] for i in unmasker(\"Python -  это самый [MASK] язык программирования.\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "task = 'feature-extraction' \n",
    "generator = pipeline(task, model=\"sberbank-ai/ruT5-base\")\n",
    "generator(\n",
    "    \"Текст: С мая 2021 в России повысят налоги. Тема: \"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeppavlov_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
